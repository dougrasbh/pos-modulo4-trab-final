{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93199b81",
   "metadata": {},
   "source": [
    "<h1>Processamento dos Dados Brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00196b8",
   "metadata": {},
   "source": [
    "## Instala√ß√£o e importa√ß√£o das bibliotecas necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a1333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ronal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ronal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ronal\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ronal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ronal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ronal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e27189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf743be",
   "metadata": {},
   "source": [
    "### üìå Etapa 1: Processamento dos Perfis HTML e Extra√ß√£o de Informa√ß√µes\n",
    "\n",
    "Com os perfis do LinkedIn salvos localmente em formato HTML, esta etapa tem como objetivo processar esses arquivos e extrair dados relevantes estruturados sobre **experi√™ncias profissionais** e **forma√ß√£o acad√™mica**.\n",
    "\n",
    "#### üîç A√ß√µes realizadas:\n",
    "\n",
    "1. **Leitura dos Arquivos HTML**\n",
    "   - Todos os arquivos na pasta `html_perfis` com extens√£o `.html` s√£o carregados e processados individualmente utilizando a biblioteca `BeautifulSoup`.\n",
    "\n",
    "2. **Extra√ß√£o de Informa√ß√µes Pessoais**\n",
    "   - O nome do perfil √© extra√≠do da tag `<h1>`.\n",
    "   - O t√≠tulo profissional (headline) √© obtido de um `div` com classe `text-body-medium`.\n",
    "\n",
    "3. **Extra√ß√£o de Se√ß√µes Espec√≠ficas**\n",
    "   - As se√ß√µes de **\"Experience\"** e **\"Education\"** s√£o localizadas com base no conte√∫do textual.\n",
    "   - Para cada item dentro dessas se√ß√µes, s√£o extra√≠dos:\n",
    "     - **T√≠tulo do cargo/curso**\n",
    "     - **Empresa/institui√ß√£o**\n",
    "     - **Per√≠odo**\n",
    "     - **Descri√ß√£o** (quando dispon√≠vel)\n",
    "\n",
    "4. **Tratamento e Estrutura√ß√£o dos Dados**\n",
    "   - Os dados extra√≠dos s√£o organizados em uma estrutura de dicion√°rio contendo os seguintes campos:\n",
    "     - `nome`, `headline`, `cargo_curso`, `empresa_instituicao`, `periodo`, `descricao`, `tipo`, `arquivo`.\n",
    "\n",
    "5. **Exporta√ß√£o para CSV**\n",
    "   - Todos os registros coletados s√£o armazenados em um `DataFrame` do pandas.\n",
    "   - O resultado √© exportado para o arquivo `perfil_linkedin_dados.csv`.\n",
    "\n",
    "#### ‚úÖ Resultado:\n",
    "- Dados organizados de forma tabular.\n",
    "- Cada linha do CSV representa uma experi√™ncia ou forma√ß√£o associada a um perfil processado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 557 perfis salvos em perfil_linkedin_dados.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "PASTA = \"../raw/html_perfis\"\n",
    "dados = []\n",
    "\n",
    "def extrair_secao_por_texto(soup, chave):\n",
    "    secoes = []\n",
    "    for section in soup.find_all(\"section\"):\n",
    "        attrs_text = \" \".join([str(val).lower() for val in section.attrs.values()])\n",
    "        section_text = section.get_text(strip=True).lower()\n",
    "\n",
    "        if chave.lower() in attrs_text or chave.lower() in section_text:\n",
    "            for li in section.find_all(\"li\"):\n",
    "                linhas = li.get_text(separator=\"\\n\", strip=True).split(\"\\n\")\n",
    "                linhas = [l.strip() for l in linhas if l.strip()]\n",
    "                if len(linhas) < 2:\n",
    "                    continue\n",
    "\n",
    "                titulo_1 = linhas[0]\n",
    "                subtitulo_1 = linhas[1]\n",
    "                periodo = \"\"\n",
    "                descricao = \"\"\n",
    "\n",
    "                for l in linhas[2:]:\n",
    "                    if any(m in l.lower() for m in [\"de \", \"to \", \"at√©\", \"present\", \"moment\", \"‚Äì\"]):\n",
    "                        periodo = l\n",
    "                    else:\n",
    "                        descricao += l + \" \"\n",
    "\n",
    "                secoes.append({\n",
    "                    \"titulo\": titulo_1,\n",
    "                    \"subtitulo\": subtitulo_1,\n",
    "                    \"periodo\": periodo,\n",
    "                    \"descricao\": descricao.strip()\n",
    "                })\n",
    "            break\n",
    "    return secoes\n",
    "\n",
    "for arquivo in os.listdir(PASTA):\n",
    "    if not arquivo.endswith(\".html\"):\n",
    "        continue\n",
    "\n",
    "    caminho = os.path.join(PASTA, arquivo)\n",
    "    with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    nome = \"\"\n",
    "    headline = \"\"\n",
    "\n",
    "    try:\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1:\n",
    "            nome = h1.get_text(strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        headline_div = soup.find(\"div\", class_=\"text-body-medium\")\n",
    "        if headline_div:\n",
    "            headline = headline_div.get_text(strip=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    experiencias = extrair_secao_por_texto(soup, \"Experience\")\n",
    "    educacoes = extrair_secao_por_texto(soup, \"Education\")\n",
    "\n",
    "    # Juntar tudo s√≥ em um texto por tipo\n",
    "    texto_exp = \" \".join([\n",
    "        f\"{e['titulo']} - {e['subtitulo']} ({e['periodo']}) {e['descricao']} |||\"\n",
    "        for e in experiencias\n",
    "    ])\n",
    "\n",
    "    texto_edu = \" \".join([\n",
    "        f\"{e['titulo']} - {e['subtitulo']} ({e['periodo']}) {e['descricao']} |||\"\n",
    "        for e in educacoes\n",
    "    ])\n",
    "\n",
    "    dados.append({\n",
    "        \"name\": nome,\n",
    "        \"headline\": headline,\n",
    "        \"experience\": texto_exp.strip(),\n",
    "        \"education\": texto_edu.strip(),\n",
    "    })\n",
    "\n",
    "if dados:\n",
    "    df = pd.DataFrame(dados)\n",
    "    df.to_csv(\"perfil_linkedin_dados.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ {len(dados)} perfis salvos em perfil_linkedin_dados.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>headline</th>\n",
       "      <th>experience</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ana Beatriz De Souza Maciel</td>\n",
       "      <td>Aluno na Escola Superior de Tecnologia-UEA</td>\n",
       "      <td>Samsung Ocean - Samsung Ocean (- Fui respons√°v...</td>\n",
       "      <td>Universidade do Estado do Amazonas - UEA - Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>√Ålef Monteiro</td>\n",
       "      <td>Angular | React | Django | Docker | PostgreSQL...</td>\n",
       "      <td>Full Stack Developer - Full Stack Developer (A...</td>\n",
       "      <td>Universidade do Estado do Amazonas - UEA - Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Janderson Chaves C. Meireles</td>\n",
       "      <td>Software Developer | Electrical Engineering | ...</td>\n",
       "      <td>Sidia Instituto de Ci√™ncia e Tecnologia - Sidi...</td>\n",
       "      <td>Universidade do Estado do Amazonas - UEA - Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Janderson Lira</td>\n",
       "      <td>Software Developer | Python | Machine Learning...</td>\n",
       "      <td>Top skills - Top skills (Python ‚Ä¢ Processament...</td>\n",
       "      <td>FGV - Funda√ß√£o Getulio Vargas - FGV - Funda√ß√£o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Janielson Moura</td>\n",
       "      <td>Desenvolvedor Fullstack</td>\n",
       "      <td>Venturus - Venturus (An√°lise de problemas, Exp...</td>\n",
       "      <td>Centro Universit√°rio Fametro - Centro Universi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Igor Rodrigues</td>\n",
       "      <td>Data Analyst | Data Scientist | Machine Learni...</td>\n",
       "      <td>Top skills - Top skills (SQL ‚Ä¢ Intelig√™ncia ar...</td>\n",
       "      <td>MBA USP/Esalq - MBA USP/Esalq (Anal√≠tica de da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Isaac Felipe dos Santos Lima</td>\n",
       "      <td>Software Developer at Localiza&amp;Co || Back-end ...</td>\n",
       "      <td>Software Developer - Software Developer (Feb 2...</td>\n",
       "      <td>Universidade do Estado do Amazonas - UEA - Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Italo Thiago</td>\n",
       "      <td>Computers Engeniring student</td>\n",
       "      <td>Webmaster - Webmaster (Feb 2025 to Present ¬∑ 5...</td>\n",
       "      <td>Universidade do Estado do Amazonas - UEA - Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Izabelly Ara√∫jo</td>\n",
       "      <td>Talent Acquisition | Tech Recruiter | Human Re...</td>\n",
       "      <td>Top skills - Top skills () Communication ‚Ä¢ Neg...</td>\n",
       "      <td>IPOG - Instituto de P√≥s-Gradua√ß√£o e Gradua√ß√£o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>J√∫lio C√©sar Santos</td>\n",
       "      <td>Consultor de Relacionamentos/Desenvolvimento d...</td>\n",
       "      <td>Gerente administrativo - Gerente administrativ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  \\\n",
       "0     Ana Beatriz De Souza Maciel   \n",
       "1                   √Ålef Monteiro   \n",
       "2    Janderson Chaves C. Meireles   \n",
       "3                  Janderson Lira   \n",
       "4                 Janielson Moura   \n",
       "..                            ...   \n",
       "552                Igor Rodrigues   \n",
       "553  Isaac Felipe dos Santos Lima   \n",
       "554                  Italo Thiago   \n",
       "555               Izabelly Ara√∫jo   \n",
       "556            J√∫lio C√©sar Santos   \n",
       "\n",
       "                                              headline  \\\n",
       "0           Aluno na Escola Superior de Tecnologia-UEA   \n",
       "1    Angular | React | Django | Docker | PostgreSQL...   \n",
       "2    Software Developer | Electrical Engineering | ...   \n",
       "3    Software Developer | Python | Machine Learning...   \n",
       "4                              Desenvolvedor Fullstack   \n",
       "..                                                 ...   \n",
       "552  Data Analyst | Data Scientist | Machine Learni...   \n",
       "553  Software Developer at Localiza&Co || Back-end ...   \n",
       "554                       Computers Engeniring student   \n",
       "555  Talent Acquisition | Tech Recruiter | Human Re...   \n",
       "556  Consultor de Relacionamentos/Desenvolvimento d...   \n",
       "\n",
       "                                            experience  \\\n",
       "0    Samsung Ocean - Samsung Ocean (- Fui respons√°v...   \n",
       "1    Full Stack Developer - Full Stack Developer (A...   \n",
       "2    Sidia Instituto de Ci√™ncia e Tecnologia - Sidi...   \n",
       "3    Top skills - Top skills (Python ‚Ä¢ Processament...   \n",
       "4    Venturus - Venturus (An√°lise de problemas, Exp...   \n",
       "..                                                 ...   \n",
       "552  Top skills - Top skills (SQL ‚Ä¢ Intelig√™ncia ar...   \n",
       "553  Software Developer - Software Developer (Feb 2...   \n",
       "554  Webmaster - Webmaster (Feb 2025 to Present ¬∑ 5...   \n",
       "555  Top skills - Top skills () Communication ‚Ä¢ Neg...   \n",
       "556  Gerente administrativo - Gerente administrativ...   \n",
       "\n",
       "                                             education  \n",
       "0    Universidade do Estado do Amazonas - UEA - Uni...  \n",
       "1    Universidade do Estado do Amazonas - UEA - Uni...  \n",
       "2    Universidade do Estado do Amazonas - UEA - Uni...  \n",
       "3    FGV - Funda√ß√£o Getulio Vargas - FGV - Funda√ß√£o...  \n",
       "4    Centro Universit√°rio Fametro - Centro Universi...  \n",
       "..                                                 ...  \n",
       "552  MBA USP/Esalq - MBA USP/Esalq (Anal√≠tica de da...  \n",
       "553  Universidade do Estado do Amazonas - UEA - Uni...  \n",
       "554  Universidade do Estado do Amazonas - UEA - Uni...  \n",
       "555  IPOG - Instituto de P√≥s-Gradua√ß√£o e Gradua√ß√£o ...  \n",
       "556                                                NaN  \n",
       "\n",
       "[557 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_linkedin = pd.read_csv(\"perfil_linkedin_dados.csv\")\n",
    "df_linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f26e9",
   "metadata": {},
   "source": [
    "### üìå Etapa 2: Cria√ß√£o e Salvamento dos Metadados do Dataset\n",
    "\n",
    "Nesta etapa, s√£o definidos e salvos os metadados associados ao dataset processado a partir dos perfis do LinkedIn. Essa documenta√ß√£o √© essencial para garantir rastreabilidade, reprodutibilidade e facilitar o entendimento e reutiliza√ß√£o dos dados por outros usu√°rios ou sistemas.\n",
    "\n",
    "#### üîç A√ß√µes realizadas:\n",
    "\n",
    "1. **Cria√ß√£o de Cat√°logo do Datalake**\n",
    "   - Um objeto `datalake_catalog` foi definido com informa√ß√µes gerais do dataset, incluindo:\n",
    "     - Nome do dataset\n",
    "     - Descri√ß√£o\n",
    "     - Caminho para o arquivo CSV processado\n",
    "     - Formato\n",
    "     - Data de cria√ß√£o\n",
    "     - Vers√£o do conjunto de dados\n",
    "   - O cat√°logo foi salvo como `datalake_linkedin_profile_data.json` na pasta `../metadata`.\n",
    "\n",
    "2. **Mapeamento dos Tipos de Dados**\n",
    "   - Foi implementada uma fun√ß√£o (`map_dtype`) para mapear os tipos de dados do pandas para uma representa√ß√£o gen√©rica (`string`, `integer`, `float`, etc.).\n",
    "   - Essa fun√ß√£o √© usada para descrever cada coluna do dataset em termos do tipo de dado.\n",
    "\n",
    "3. **Defini√ß√£o Detalhada de Metadados**\n",
    "   - Os metadados incluem:\n",
    "     - Nome do arquivo\n",
    "     - Descri√ß√£o do dataset\n",
    "     - Formato, codifica√ß√£o e separador de campo\n",
    "     - N√∫mero total de linhas\n",
    "     - Lista de colunas com nome, tipo e descri√ß√£o\n",
    "     - Origem da coleta\n",
    "     - Data da coleta\n",
    "     - Respons√°veis pelo projeto (`owner`)\n",
    "\n",
    "4. **Armazenamento**\n",
    "   - Os metadados completos foram salvos no arquivo `processed_data_metadata.json`, tamb√©m dentro da pasta `../metadata`.\n",
    "\n",
    "#### ‚úÖ Resultado:\n",
    "Dois arquivos JSON contendo os metadados do dataset:\n",
    "- `datalake_linkedin_profile_data.json`: para fins de cat√°logo no datalake.\n",
    "- `processed_data_metadata.json`: descri√ß√£o t√©cnica detalhada do conte√∫do do dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c11bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalake_catalog = [\n",
    "  {\n",
    "    \"dataset_name\": \"Linkedin Profiles Data\",\n",
    "    \"description\": \"Dataset generated from LinkedIn profiles, containing professional experiences and education.\",\n",
    "    \"path\": \"../processed/perfil_linkedin_dados.csv\",\n",
    "    \"format\": \"csv\",\n",
    "    \"created_at\": str(date.today()),\n",
    "    \"version\": \"1.0\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6acc0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_path = Path('../metadata')\n",
    "catalog_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = catalog_path / \"datalake_linkedin_profile_data.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(datalake_catalog, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36ab4f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'name', 'type': 'string', 'description': 'string'},\n",
       " {'name': 'headline', 'type': 'string', 'description': 'string'},\n",
       " {'name': 'experience', 'type': 'string', 'description': 'string'},\n",
       " {'name': 'education', 'type': 'string', 'description': 'string'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_string_dtype(dtype):\n",
    "        return \"string\"\n",
    "    elif pd.api.types.is_integer_dtype(dtype):\n",
    "        return \"integer\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"float\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return \"boolean\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return \"datetime\"\n",
    "    else:\n",
    "        return \"string\"\n",
    "    \n",
    "columns_info = [\n",
    "    {\"name\": col, \"type\": map_dtype(dtype), \"description\": \"string\"}\n",
    "    for col, dtype in df_linkedin.dtypes.items()\n",
    "]\n",
    "\n",
    "columns_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca1442e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadados = {\n",
    "  \"file\": \"perfil_linkedin_dados.csv\",\n",
    "  \"description\": \"Dataset containing professional experiences and education extracted from LinkedIn profiles.\",\n",
    "  \"format\": \"csv\",\n",
    "  \"separator\": \",\",\n",
    "  \"encoding\": \"utf-8\",\n",
    "  \"number_of_rows\": len(df_linkedin),\n",
    "  \"columns\": \n",
    "    [{'name': 'name', 'type': 'string', 'description': 'Person name'},\n",
    "      {'name': 'headline', 'type': 'string', 'description': 'Professional headline'},\n",
    "      {'name': 'experience', 'type': 'string', 'description': 'Professional experiences'},\n",
    "      {'name': 'education', 'type': 'string', 'description': 'Education details'}],\n",
    "  \"origin\": \"Unknown\",\n",
    "  \"collect_date\": \"2025-06-19\",\n",
    "  \"owner\": \"Ronald Boadana, Debora Barros and Arthur Santos\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f854ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"../metadata/processed_data_metadata.json\"\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(metadados, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
